---
title: "Mapping and Spatial Analysis in R"
author: "Michael Fichman"
date: "CPLN 6750, Spring, 2025"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(rmarkdown)
```

# Introduction

This tutorial will introduce you to the use of the `sf` package for vector-based spatial analysis and mapping. This code uses the `tigris` and `tidycensus` packages to query the US Census Bureau's databases of demographic and spatial information. It also integrates `sf` into the `ggplot2` data visualization workflow to create vector maps which visualize spatial information.

The "use case" for this exercise is a spatial analysis in which the locations and loads of toxic air polluting facilities are related to the nearby concentrations of school age children. 

How many children should Allegheny County (PA) schools be prepared to handle in the upcoming years who have been exposed to toxic air emissions? We will assume that there is some relationship between emissions exposure and learning disabilities school districts should be ready to allocate resources to districts with the greatest need for special education.

We want to know where all the toxic facilities are in Allegheny County and map them alongside a visualization of the percentage of pre-school aged children nearby. Additionally we want to know what school districts the facilities are in and how many children they can expect to have been exposed to emissions from age 0-5 during the years 2010-2015.

This tutorial takes an estimated 90 minutes to complete.

# Skills Learned in This Exercise

This exercise consists of a few classic planner's tasks:

1. Downloading spatial data from government APIs and open data sites

2. Importing and exporting shapefiles

3. Transforming data (e.g. changing its projection)

4. Creating and styling maps using `ggplot2` and `sf` with hydrology, labels and symbologies

5. Joining spatial data together by location

6. Summarizing point data by polygon geographies

# Workflow overview

We are going to load in the following data sets:

1. US census data about populations of children aged 0-5.

2. PA data about events of toxic chemical emission, including date, amount, type and location.

3. School district boundaries.

We will then summarize the number and volume of chemical emissions by school district using a point-in-polygon join, and we will summarize the number of 0-5 children in each district by joining tract centroids to school district polygons.

Lastly, we will calculate a per-school and per-pupil volume of emissions by district.


# Resources

THere are lots of resources on the web you can consult to make `sf` and `ggplot2` work better for you. There are great ["cheat sheets"](https://github.com/rstudio/cheatsheets/raw/master/sf.pdf) describing the capabilities of the sf package and [helping you style maps](https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html).

We are going to be doing reprojection of data when it suits us.   Each projection has an identifying number (a "crs" or coordinate reference system) which is listed at [spatialreference.org](http://spatialreference.org/).

# Setup 

## Install Libraries

If you haven't installed any of the folling packages, use the `install.packages` command to apply them as needed. You should have installed everything but `tigris` and `viridis` during Class 7.

(If you have them installed, you can skip this step)

```{r setup_packages1, warning = FALSE, eval = FALSE}
install.packages('tidyverse')
install.packages('tidycensus')
install.packages('sf')
install.packages('tigris')
install.packages('viridis')
```

Once the packages are installed, you must load them using the `library` command so that they are active in your environment.

```{r setup_packages2, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidycensus)
library(sf)
library(tigris) # package to download tiger shapefiles from census API
library(viridis)
```

## Census API Key

You will need a "key" to access the Census API. You can find one at [their website](https://api.census.gov/data/key_signup.html).

Paste it into the code block below:

```{r load_key_hide, warning= FALSE, include=FALSE}
census_key <- read.table("~/GitHub/census_key.txt", quote="\"", comment.char="")
census_api_key(census_key[1] %>% as.character(), overwrite = TRUE)
```

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("YOUR KEY GOES HERE", overwrite = TRUE)
```

## Set Tigris Options

We want to set some options so that when we download data from the Census Bureau's Tigris spatial database, we get our data in `sf` format and we "cache" our downloads so they are stored on our machine.

```{r tigris_options, warning = FALSE, cache = TRUE}
options(tigris_class = "sf")
options(tigris_use_cache = TRUE)
```

# Loading Spatial Data With Tidycensus

## Load census data dictionaries

Now that we have our census credentials loaded, we can start downloading information from the API using some functions from tidycensus. We are going to make some comparisons between 2016 and 2010 ACS estimates for our target census tracts. In order to choose variables of interest, we are going to load the data dictionaries for each time period using `load_variables`.

What does this `load_variables` function do and why are we writing these things in the parentheses after the name of the function? If you want to know about a function in R, you can type the name of a package or function into the console like this: `??load_variables` and information will show up in the help window in your R Studio environment.

```{r load_variables, warning = FALSE, cache = TRUE}
acs_variable_list.2016 <- load_variables(2016, #year
                                         "acs5", #five year ACS estimates
                                         cache = TRUE)
```

Once we have loaded these data frames, we can observe and search through the data frames of variable information which should appear in our global environment either by clicking on them or using the `View(YOUR DATAFRAME NAME HERE)` command.

Let's look around in these data frames for a few minutes and see what's in there.

## Create a vector of census variables

We can populate a vector of variable names we will send to the Census API. We call this list `acs_vars`. This is the beauty of a code-based workflow - you can take this vector and put anything you want in it when you have a new analysis to do and re-run it for different variables. These need to be character strings, and hence, in quotes as you see below.

Keep in mind the categories and code numbers change a bit over time - you may need separate vectors for different census years.

Le create a vector called `acs_vars` which contains three census variables we are interested in - persons under 5 years old and a total population estimate.

```{r acs_vars, warning = FALSE, cache = TRUE}
acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B01001_003E", # Male:!!Under 5 years
              "B01001_027E") # Female:!!Under 5 years
```

## Call the Census API using tidycensus

We use the `get_acs` function in `tidycensus` to query the API. Notice the different arguments for the function, and that they require certain types of info. For example, `geography` requires one of a finite list of answers, and they have to be formatted as character string.

Remember the `??` function - you can learn about the parameters for `get_acs` this way. There is also a function called `get_decennial` which you can use for decennial census counts.

We ask for data on our `acs_vars` for all tracts in Allegheny County, PA in 2016. We ask for "wide" data (e.g. one variable per column, one row per tract) and we set `geometry` to `TRUE`.

```{r get_acsTracts.test, cache = TRUE, warning = FALSE, message = FALSE, results = "hide"}
acsTracts.2016 <- get_acs(geography = "tract",
                             year = 2016,
                             variables = acs_vars,
                             geometry = TRUE,
                             state = "PA",
                             county = "Allegheny",
                             output = "wide")
```

Let's examine these data using the `glimpse()` command to see how they look and what the data types are. 

```{r glimpse_acsTracts.2016, cache = TRUE}
glimpse(acsTracts.2016)
```

## Mutating, selecting and renaming variables

Now we can manipulate our data using some of our `tidyverse` data wrangling tools from the `dplyr` library.

The `dplyr` package is great for these operations and has some very common sense functions that are fairly intuitive because they use verbs. You can select columns (`select`), rename columns (`rename`), summarize data (`summarize`) by groups (`group_by`), doing arithmetic across columns (`mutate`).

The operator `%>%` is known as the "pipe" and lets you chain operations together - passing data along through different operations.

Recall the example from [Intro to R For Planners](https://mafichman.github.io/R_FAQ_For_Planners/) about how the pipe is used to operate on data - making a new data frame, called omlette, out of some eggs (our original data frame) and some actions:

`omlette <- eggs %>% scramble() %>% fry()`

Let's look at a simple example of the use of each "verb", one step at a time so we can remember what they do:

`select`

Let's create a new data frame called `df_3_variables` by selecting only the GEOID, NAME, and our `acs_vars` - we are only using the estimates from the ACS (suffix "E" on the variable names), we are not retaining the margin of error calculations (suffix "M" on the variable names).

```{r}
df_3_variables <- acsTracts.2016  %>%
  select (GEOID, NAME, acs_vars)
```

`rename`

I can rename my census variable codes to be something more intelligible based on their codebook definitions.

```{r}
df_3_variables_renamed <- df_3_variables %>%
  rename (total.2016 = B01001_001E,
          male.under.5 = B01001_003E,
          female.under.5 = B01001_027E)

```

`mutate`

I can create new columns with `mutate` and sum the number of students into a single population column and calculate the percentage of the total tract population that sub-population constitutes.

```{r}
df_selected_renamed_mutated <- df_3_variables_renamed %>%
  mutate(total.under.5 = male.under.5 + female.under.5,
         pct.under.5 = 100*(total.under.5/total.2016))

```

OK, that was fun - if we know our workflow, we can do it all at once and spare ourselves the inconvenience of creating tons of new data frames in our environment.

```{r do_some_dplyr, warning = FALSE, cache = TRUE, results = "hide"}
acsTracts.2016 <- acsTracts.2016  %>%
  select (GEOID, NAME, acs_vars) %>% # select just the GEOID and 
  rename (total.2016 = B01001_001E,
          male.under.5 = B01001_003E,
          female.under.5 = B01001_027E) %>%
  mutate(total.under.5 = male.under.5 + female.under.5,
         pct.under.5 = 100*(total.under.5/total.2016))
```

## Converting to Simple Features

The `sf` package is a great way to deal with vector data - you can treat the data like a data frame for data wrangling operations, and map it using the powerful `ggplot` package. `sf` will recognize that the `geometry` column in our data is a set of instructions for drawing polygons.

We call the `st_as_sf` function to coerce these spatial data to `sf` format and `st_transform` to project them to the WGS84 Web Mercator coordinate system which is numbered `crs = 4326`.

```{r convert_to_sf, warning = FALSE, cache = TRUE}
acsTracts.2016 <- acsTracts.2016  %>%
  st_as_sf() %>% # convert the shapefile to an "sf" object
  st_transform(crs=4326) # transform the data to the "web mercator" projection
```


## Using dplyr and tidy syntax to code more efficiently

Let's try redo everything we just tried, only this time we will use a more efficient code routine, and do all the data intake at once. You can set up your own workflows like this for routines you expect to do repeatedly.

We are going to load and wrangle our data in one big series of functions chained together by the `%>%` operator. If you cant to get under the hood, you can just run portions of the call in your console window and see what the output looks like at each step.

We call the census API using `tidycensus` function `get_acs` for tract-level data representing our `acs_vars` for Allegheny County, PA using the 2016 five year estimates. 

We set our geometry to `TRUE` so we get a shapefile attached. 

We `select` only the GEOID, tract name and our `acs_vars` to remove our margins of error. We `rename` our variables to make them more intelligible.

We then `mutate` (e.g. create new variables) to tabulate the total population under 5 years old, both as a gross count and as a percentage.

Lastly we call the `st_as_sf` function to coerce these spatial data to `sf` format and `st_transform` to project them to the WGS84 Web Mercator coordinate system which is numbered `crs = 4326`.

```{r get_acsTracts.2016,  warning = FALSE, message = FALSE, results = "hide"}
acsTracts.2016 <- get_acs(geography = "tract",
                             year = 2016,
                             variables = acs_vars,
                             geometry = TRUE, # This is different from what we've been doing!
                             state = "PA",
                             county = "Allegheny",
                             output = "wide") %>%
  select (GEOID, NAME, acs_vars) %>% # select just the GEOID and our key variable names
  rename (total.2016 = B01001_001E,
          male.under.5 = B01001_003E,
          female.under.5 = B01001_027E) %>%
  mutate(total.under.5 = male.under.5 + female.under.5,
         pct.under.5 = 100*(total.under.5/total.2016)) %>%
  st_as_sf() %>% # convert the shapefile to an "sf" object
  st_transform(crs=4326) # transform the data to the "web mercator" projection
```

We now have this census data with a geometry, and we can manipulate sf objects just like data frames. We can export them as shapefiles or remove the geometry and export them as csv files.

Use the `glimpse()` function to take a look at the data - you can do this in your console window.

# Mapping Polygons

Mapping polygons is easy with `ggplot2` and `sf` - we can just add a `geom_sf` to a ggplot!

```{r map1, cache = TRUE}
ggplot()+
  geom_sf(data = acsTracts.2016)
```

We can get more and more complex from here. Let's add a `fill` aesthetic and visualize the `pct.under.5` by tract.

```{r map2, cache = TRUE}
ggplot()+
  geom_sf(data = acsTracts.2016, 
          aes(fill = pct.under.5))
```

Let's keep adding on. We can make the `color` of our linework "transparent" to make things less busy.

And let's make the color ramp a bit nicer using the `viridis` package where we can send styling info to the `fill` command we have in our `aes` call.

Wait, how did we do that? Type `??scale_fill_viridis` in your console in R Studio to access the documentation - 

```{r map3, cache = TRUE}
ggplot()+
  geom_sf(data = acsTracts.2016, 
          aes(fill = pct.under.5), 
          color = "transparent")+
   scale_fill_viridis('Pct Under 5', # Scale label
                      direction = -1, # Direction of the color palette - either 1 or -1
                      option = 'D',  # Viridis has multiple palettes to choose from
                      alpha = 0.6) # Transparency level
```

*Exercise:*

Can you make a version of the map above with a title and labels?

Use the `labs` in ggplot to add a title and subtitle for the map.


# Tigris Data

Let's add some more data to our map using `tigris`, the package which calls the Census' shapefile repository. The Census Bureau keeps lots of data that isn't associated with the Census survey itself.

Let's load the county boundary shapefile using the `counties` function, transform it to an sf object (st_as_sf), st_transform it it to `crs=4326` (web mercator). Because we can only get counties a fill state at a time, we can then `filter` for just the county we are interested in.

If you want to see what the intermediate products are in this chain of functions, just run the first line or first two lines and then examine the object that's created to see how it's shaped step by step here.

```{r counties_tigris, message = FALSE, warning=FALSE, results = "hide"}
allegheny <- counties('PA') %>%
  st_as_sf()%>%
  st_transform(crs=4326)%>%
  filter(NAME == "Allegheny")
```

Let's add some water (for graphics funtimes) by getting another shapefile from tigris using a similar routine.

```{r area_water, warning = FALSE, message = FALSE, results = "hide"}
water <- area_water('PA', county = 'Allegheny') %>%
  st_as_sf()%>%
  st_transform(crs=4326)
```

# Spatial Joins

Let's load the unified school districts and get them set up as an `sf` object projected to `crs=4326`. We start with all the PA districts.

In order to keep only those districts which are within our county boundary shapefile, we do a spatial join using the `sf` function `st_join` - we only want districts that are within (`st_within`) our county boundary. We say we do not want to do a "left join" - in this join the state-wide districts are the left side of the join (functionally the first data file called) - we only want to keep what joins from the "right".

Note that when we do the join - the `st_join` function is looking for two files to join - since we "pipe" data into the funtion, the `.` stands in for the data being piped.

Why do you suppose this type of join gives us the output we want? Can you think of an alternative method for joining these data?


```{r unified_districts, cache = TRUE, warning = FALSE, message = FALSE, results = "hide"}
unified.districts <- school_districts('PA', type = "unified") %>%
  st_as_sf() %>%
  st_transform(crs=4326) %>%
  st_join(., allegheny, # do a join just keep those districts "within" the allegheny county boundary
          join = st_within, # this is the spatial join type from sf
          left = FALSE) # by specifying no "left" join - we keep only the allegheny county observations
```

Let's take a look at `unified.districts` using the `glimpse` command.

Notice that we have some ugly looking stuff in there - there are ".x" and ".y" appendages to everything that had an equivalent column name on either side of the join.

Let's clean up a bit and `select` the columns we want to keep and `rename` them.

We can also add columns for the lat/lon of the "centroid" points for each district which we can use to label the features later when we are making graphics.


```{r clean_unified_districts, cache = TRUE, warning = FALSE, message = FALSE}
 unified.districts <- unified.districts %>% 
  select(NAME.x, GEOID.x) %>% # let's just keep the variables we need - district name and geoid
  rename(NAME = NAME.x) %>% # both files had a "NAME" column, so the join turned one into "NAME.x" - let's rename it
  mutate(lon=map_dbl(geometry, ~st_centroid(.x)[[1]]), # add centroid values for labels
         lat=map_dbl(geometry, ~st_centroid(.x)[[2]])) # add centroid values for labels
```

# More Mapping

Let's add the to our map and make the styling better. We can use the aesthetics from a ggplot theme `theme_void` to create a more spare visualization. Let's change the options on `scale_fill_viridis` to keep our color ramp from conflicting with the blue water we're going to add.

Notice we are adding several `geom_sf` objects here and it's therefore important they be in the same coordinate system. We use WGS84 so much because we can use point data very easily which contains lat/lon (more on that later).

Let's also add some labels using the `labs` option.

```{r nice_map1, cache = TRUE}
ggplot()+
  geom_sf(data = acsTracts.2016, 
          aes(fill = pct.under.5), 
          color = "transparent")+
  scale_fill_viridis('Pct 0-5 year olds', # Scale label
                     direction = -1, # Direction of the color palette - either 1 or -1
                     option = 'D',  # Viridis has multiple palettes to choose from
                     alpha = 0.6)+ # Transparency level
  geom_sf(data = unified.districts,
          color = "white",
          fill= "transparent")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+ # let's make the boundary thick
  labs(
    title = "Distribution of pre-schoolers in Allegheny County by census tract",
    subtitle = "White polygons represent unified school districts",
    caption = "Data: US Census Bureau, 2016 ACS"
  )+
  theme_void()
```

If we wanted, we could output this map - either by using the `export` option in the Plot viewer or using `ggsave` - we can do this in pdf or eps file format and manipulate it in illustrator if we like.

# Loading Spatial Data from File

Let's go to WPC's open data site and download a csv of toxic release points in Allegheny County.

[https://data.wprdc.org/dataset/toxic-release-inventory/resource/b0c22ad4-7afb-496b-9500-3aebf91e67d6](https://data.wprdc.org/dataset/toxic-release-inventory/resource/b0c22ad4-7afb-496b-9500-3aebf91e67d6).

If you can't find it there, it's available at my github:

[https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_5/data/toxic.csv](https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_5/data/toxic.csv)

Then, read it in using either a line of code leading to the filepath (as below) or using the Import command in the "File" dropdown in R Studio

```{r load_toxic, cache = TRUE}
toxic <- read.csv("~/GitHub/CPLN_675/Week_5/data/toxic.csv")
```


Let's take a look at these using the `glimpse()` command.


Let's filter these data to just use toxic release events since 2010.


```{r filter_toxic, cache= TRUE}
toxic.5 <- toxic %>%
  filter(YEAR > 2010,
         COUNTY == "ALLEGHENY")
```

## Exploring the Toxic Release Data

*Exercise*

Using `group_by`, and `tally` - can you tell me what the most common `CHEMICAL` is in this data set? What about the most frequent `FACILITY_NAME` associated with releases?

## Mapping our toxic release data

We don't need to turn this into an sf object at the moment. All our other data are projected in web mercator, which uses the same spatial reference as standard lat/lon values - so we can plot these using `geom_point` which is far faster to render. (You could go the `sf` route if you want, this is just for demonstration purposes.)


Let's visualize these points on the map alone. We can simply use geom_point if we are using unprojected, lat/lon data.

This is just like the last map but with points on top.

```{r nice_map2, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5,
          size = 1)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+
  geom_point(data = toxic.5, 
             aes(x=LONGITUDE, y=LATITUDE), 
             color = 'red')+
  labs(
    title = "Toxic Release Points in Allegheny County, 2010-2015",
    subtitle = "Probably not good that they are all by the rivers",
    caption = "Data: US Census Bureau, Allegheny County"
  )+
  theme_void()
```

# Summarizing Emission Data

We have 2047 observations but far fewer points, this means there are multiple releases per site. Let's summarize each site by number of releases and gross tonnage of released material. This requires us to `group_by` the `FACILITY_NAME` (and also the lat/lon so we can keep these pieces of info).

```{r toxic_5_summary, cache = TRUE, warning = FALSE, message = FALSE}
toxic.5.summary <- toxic.5 %>%
  group_by(FACILITY_NAME, LATITUDE, LONGITUDE)%>%
  summarize(emissions.tons = sum(ON.SITE_RELEASE_TOTAL)/2000,
            n = n())
```

Let's make a plot with some graduated points to represent the volume of emissions

```{r nice_map3, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5,
          size = 1)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+
  geom_point(data = toxic.5.summary, 
             aes(x=LONGITUDE, y=LATITUDE, size = emissions.tons), 
             color = 'red',
             alpha = 0.6)+
  labs(
    title = "Toxic Release Points in Allegheny County, 2010-2015",
    subtitle = "Probably not good that they are all by the rivers",
    caption = "Data: US Census Bureau, Allegheny County"
  )+
  theme_void()
```

# Making labels

Let's label the school districts (this ends up being kinda ugly)

```{r nice_map_labels1, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5,
          size = 1)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+
  geom_point(data = toxic.5.summary, 
             aes(x=LONGITUDE, y=LATITUDE, size = emissions.tons), 
             color = 'red',
             alpha = 0.6)+
  geom_text(data=unified.districts, aes(x=lon, y=lat, label=NAME), alpha = 0.75, size = 2)+
  labs(
    title = "Toxic Release Points in Allegheny County, 2010-2015",
    subtitle = "Subtitle",
    caption = "Data: US Census Bureau, Allegheny County"
  )+
  theme_void()
```

*Exercise*

Let's try to do this again but remove the word "School District" from the NAME field - we can do this dynamically without even touching or altering the data set.

Using the code above, can you insert a piped command of the `str_remove` command (a part of the `stringr` package in the tidyverse) into the `label` argument of the geom_text call to chop off the phrase "School District" from each string?

In the end it will look like this:

```{r nice_map_labels2, echo=FALSE, cache=TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5,
          size = 1)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 1)+ # let's reduce the boundary thickness
  geom_point(data = toxic.5.summary, 
             aes(x=LONGITUDE, y=LATITUDE, size = emissions.tons), 
             color = 'red',
             alpha = 0.6)+
  geom_text(data=unified.districts, 
            aes(x=lon, y=lat, 
                label=NAME %>%
                      str_remove("School District")), 
            size = 2)+
  labs(
    title = "Toxic Release Points in Allegheny County, 2010-2015",
    subtitle = "Subtitle",
    caption = "Data: US Census Bureau, Allegheny County"
  )+
  theme_void()
```

# Spatial Joins - Points in Polygons

Now let's use spatial statistics to create summaries of both toxic emissions and demographics by school district.

First we will do a spatial join of points to polygons. We know we need to turn the `toxic.5.summary` dataframe into an sf object to make this work (`sf` spatial operations require `sf` objects).

We can do it *inside* the st_join function and give it the correct crs.

```{r st_join_points, cache = TRUE, warning = FALSE, message = FALSE}
districts.and.pollution <- st_join(unified.districts, toxic.5.summary %>%
                                                      st_as_sf(crs=4326, 
                                                      coords = c("LONGITUDE", "LATITUDE")))
```

Let's see what this looks like using the call `View(districts.and.pollution)`

Let's repeat the join we just did, but let's do some summary statistics - it's always nice to be efficient and chain functions together rather than throwing off intermediate dataframes and sf objects.

We group points by the name of the district (`NAME`) and summarize the total emissions and number of emission events per district (`sum(n)`). We use the `na.rm = TRUE` function to remove NA observations - districts with no emissions will trip up our summary calculations.

```{r st_join_districts2, cache = TRUE, warning = FALSE, message = FALSE}
districts.and.pollution <- st_join(unified.districts, toxic.5.summary %>%
                                  st_as_sf(crs=4326, 
                                               coords = c("LONGITUDE", "LATITUDE"))) %>%
  group_by(NAME) %>%
  summarize(emissions.tons = sum(emissions.tons, na.rm = TRUE),
            emissions.events = sum(n, na.rm = TRUE))
```

We can create a per-pupil emissions statistic to get more "normalized" measures. This means we have to join our tracts to our districts. We are going to do a centroid-based point-in-polygon join here - we want to avoid any modifiable aerial unit problems with our tracts. It turns out districts and tracts are NOT 100% coterminous. This is a complex exercise.

We calculate centroid points for our tracts in order to do this points-to-polygons join - we do this using the sf command `st_centroid`.

```{r point_in_poly2, cache = TRUE, warning= FALSE, message = FALSE }
acs.point <- acsTracts.2016 %>%
  st_centroid()
```

Explore these data using the `glimpse()` command. Make a simple `ggplot` to check them out.

```{r lil_ggplot, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = acs.point)
```

Now we can join points to polygons (tracts to districts) and summarize total population under 5 and the emissions per pupil by district.

```{r final_join, cache = TRUE, warning = FALSE, message = FALSE}  
districts.and.tracts <- st_join(districts.and.pollution, acs.point) %>%
  group_by(NAME.x, emissions.tons, emissions.events) %>%
    summarize(total.under.5 = sum(total.under.5),
              total.pop = sum(total.2016),
              pct.under.5 = 100*(sum(total.under.5) / sum(total.2016))) %>%
  mutate(emissions.per.pupil = ifelse(is.na(emissions.tons) == FALSE, 
                                      (emissions.tons*2000) / total.under.5, 
                                      0))
```

# Mapping Final Analysis

Let's take a quick look at the distribution of our emissions-per-pupil calculation by making a histogram.

```{r quick_histogram, cache = TRUE, message = FALSE, warning = FALSE}
ggplot(data = districts.and.tracts)+
  geom_histogram(aes(emissions.per.pupil), bins = 100)
```

*Exercises*


1. There seems to be one big outlier, can you create a new histogram where you filter the `districts.and.tracts` data inside the ggplot call to contain only `emissions.per.pupil` less than 2000?  While you're at it, can you add labelling using `labs` and a slicker looking ggplot theme (look up some nice ones online)?


2. Can you look at the code for map below and write a sentence or two deciphering the purpose of some things in the code? What is going on in the `fill` command in `geom_sf`. How about the `labels` command in `scale_fill_viridis`? Why is there an `arrange` command in `geom_text`?

```{r final_map, cache = TRUE, message = FALSE, warning = FALSE}
ggplot()+
  geom_sf(data = districts.and.tracts, 
          aes(fill = ntile(emissions.per.pupil, 4)))+
  scale_fill_viridis('Lbs per person - quintile breaks', # Scale label
                     direction = -1, # Direction of the color palette - either 1 or -1
                     option = 'D',  # Viridis has multiple palettes to choose from
                     alpha = 0.6,  # Transparency level
                     labels=as.character(quantile(round(districts.and.tracts$emissions.per.pupil, 2),
                                                  c(.25,.5,.75,1))))+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+ # let's make the boundary thick
  labs(
    title = "Toxic emissions per persons 0-5 years old, 2010-2015, by school district",
    subtitle = "Some districts have zero emissions, a few have a very large volume. Top 10 districts are labelled.",
    caption = "Data: US Census Bureau, 2016 ACS, State of PA"
  )+
  geom_text(data= districts.and.tracts %>%
              mutate(lon=map_dbl(geometry, ~st_centroid(.x)[[1]]), # add centroid values for labels
                     lat=map_dbl(geometry, ~st_centroid(.x)[[2]])) %>%
              ungroup()%>%
              arrange(-emissions.per.pupil )  %>% 
            slice(1:10), 
            aes(x=lon, y=lat, 
                label=NAME.x %>%
                str_remove("School District")), 
            color = "white",
            size = 2)+
  theme_void()

```

# Exporting and importing shapefiles

You can write out any sf objects as .shp files which are readable in ArcGIS. This is especially useful if you want to just use R to grab data efficiently and want to map it elsewhere.

Use the fuction "st_write"

`??st_write`

You can also read in shapefiles with an `shp` file extension using `st_read`.

# Bonus - Making an R Markdown

Check out the button at the top right of this markdown - you can download the .rmd file and then look at the markdown code underneath. Can you add a code chunk and then "knit" your own Markdown?

# Bonus - Spatial Data with mapview

The `mapview` package is a handy tool for making interactive maps with `sf`.

If you don't have the package installed, install it with `install.packages("mapview")` prior to running the code below:

```{r load_mapview, warning = FALSE, message = FALSE}
library(mapview)
```

```{r mapview_emissions}
mapView(districts.and.tracts, zcol = "emissions.per.pupil")
```