---
title: "CPLN 675: Interpolating Air Pollution Dynamics with Interpolation and Regression"
author: "Michael Fichman"
date: "2/19/2023"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE,message = FALSE,cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(knitr)
```


***This markdown is based on material created cooperatively with the late Prof. Ken Steif.***

# 1. Introduction

Weather, air pollution and elevation are typically represented as continuous spatial processes. However, they are often derived from discrete sample points like weather stations, air quality sensors and GPS units and *interpolated* into surfaces. This module teaches students how convert discrete samples of ozone to continuous spatial estimated ‘surfaces’. It also introduces regression-based geospatial predictive modeling.

The first part of the module contains instructions on how to conduct interpolation using kriging and inverse distance weighting procedures in ArcGIS and to calculate spatial lags.

The second part of the module involves running a regression model in R to estimate ozone levels.

## 1.2. Learning Objectives

- Understand the idea of spatial interpolation. Compare and contrast methodological approaches.

- Use the "fishnet" as a vector alternative to raster GIS.

- Introduction to geospatial predictive modeling: run some linear models and see how they work mechanically when used for prediction.

# 2. Data

Our operations in ArcGIS will use the `Mid_Atlantic_EPA_Dataset.shp` file. Open these data in ArcGIS and explore them. 

The `ozoneHigh` field is the highest measure of ozone sampled for 2016 at a given grid cell throughout the mid-Atlantic United States. Ozone is good in the stratosphere where it protects Earth from the Sun’s ultraviolet rays, but bad at ground-level because of its effects on respiratory health.

Grid cells denoted by `training == 1` are those areas where EPA maintains an ozone air quality sensor. The purpose of this first exercise is to predict ozone for all the locations where there are no sensors.

## 2.1. The Fishnet

These grid cells are a "fishnet" (which is an ESRI term) and we use them to approximate a raster approach to GIS while still having access to the tabular data that we like from vector GIS.

To create a fishnet in ArcGIS, there's a handy tool called "Create Fishnet" where you input a study area and specify a cell size. To create a fishnet in R (a skill you might want to apply on your midterm), consult the Appendix at the end of this markdown.

# 3. Interpolation in ArcGIS

The Arc module consists of the following steps:

- In ArcGIS - map `ozoneHigh` and some of the other other features in the data

- Create spatial lag of `ozoneHigh` in ArcGIS. 

- Create an IDW prediction in the `Mid_Atlantic_EPA_Dataset`.

- Create a Kriging prediction in the `Mid_Atlantic_EPA_Dataset`.

Start by visualizing `ozoneHigh` and seeing the spatial contours of the data.

## 3.1 Calculate a spatial lag in ArcGIS

Now that we’ve seen some more traditional interpolation techniques, let’s see what a regression can do for us.

We’re going to begin with some instructions on how to create the spatial lag variable in ArcGIS. This is the variable that will allow us to model the spatial autocorrelation. It is defined as the average ozoneHigh of any locations k nearest training neighbors.

1. Create a new shapefile of just the training grid cells, called `training`. 
- Make sure you save this new shapefile to your project's folder or geodatabase.

2. We are going to measure nearest neighbor distance from our fishnet cells to the training set. Using the ‘Generate Near Table’ tool, set the `Mid_Atlantic_EPA_Dataset` as the Input Features (make sure you have unselected any features you might have selected in the last step!) and use `training` as the Near Features. 

3. To specify how many nearest neighbors to measure distance to, uncheck ‘Find only closest feature’ and set ‘Maximum number of closest features to 3’. Call this new table `nearTraining`.

- Note that the new table has the distance from each `IN_FID` from the Mid-Atlantic shapefile to its nearest `NEAR_FID` from the training shapefile. Note there are three entries for each `IN_FID` because we chose 3 nearest neighbors.
- Before proceeding to step 4, you may need to bring in your `training` shapefile from your project's folder or geodatabase (in some cases, the creation of the new `neartraining` table may have removed it from your contents pane).


4. We’re not interested in the distances per se - we want to get the average high ozone observation for each of the three nearest grid cells. We join the original `training` shapefile to the `nearTraining` using `NEAR_FID` from the latter to join to `OBJECTID` from the former (this might also be called `FID`, depending on your version of ArcGIS). The join adds new columns to the `training` shapefile. Now we know the `ozoneHigh` of each training grid cell where there’s an air quality sensor.

5. Export this joined table to its own `.dbf` file or geodatabase table and call it `nearTraining2`.
- Note that you can export the joined table directly from your contents pane, or you can use the "Table to Table" tool.

6. We’re going to calculate the average `ozoneHigh` of each cell’s 3 nearest training neighbors. Add `nearTraining2` to your viewer; open the attribute table; right click on IN_FID and click `summarize`. Drop down the `ozoneHigh` field and make the statistic type `MEAN`. Name this table `nearTraining3` and run the tool.

7. Note that `nearTraining3` has 1,025 rows (one for each cell in the original Mid-Atlantic shapefile) and an average `ozoneHigh` calculation. If you have the wrong number of cells...

8. Join `nearTraining3` to the original Mid-Atlantic shapefile and map the `average_OzoneHigh` (this field also may be called `mean_OzoneHigh`).

9. Export this shapefile as `Mid_Atlantic_EPA_Dataset_withLag` Move this shapefile into R (see section 4.1 of this Markdown). If you can't complete this routine, you can catch up by using a similarly named data set found in the course github (the link to this data set, as a geojson, is below.)

## 3.2. IDW in ArcGIS

10. Use the `IDW` tool to create surface of estimated Ozone. Select `training` as your input features, and the `ozoneHigh` as the "Z-Value" field. Choose a raster cell size and extent that "make sense" for this analysis - it's your call as to what that is. You don't need to select a "Geostatistical Layer." Call this output `idw_1`

11. Use `Zonal Statistics as Table` to summarize `idw_1` by `Mid_Atlantic_EPA_Dataset` using the Zone Field `FID`. Choose statistics type "MEAN" and call the output `idw_zonal`.

12. Output this table as `idw_zonal.csv`.

## 3.3. Kriging in ArcGIS

13. Use the "Feature to Point" tool on `training` to get data for these sensors at point level. Call these points `kriging_centroid`.

14. Use the 'kriging' tool to create a surface. Input features are `kriging_centroid`, Z-Value Field is `ozoneHigh`. Set the cell size to be the same as `idw_1`. Let's leave the default parameters alone - do we really know anything as a class about the decay functions for Ozone?  Let's call the output `krige_1`

15. Use `Zonal Statistics as Table` to summarize `krige_1` by `Mid_Atlantic_EPA_Dataset` using the Zone Field `FID`. Choose statistics type "MEAN" and call the output `krige_zonal`.

16. Output this table as `krige_zonal.csv`.


# 4 Exploratory Analysis

We'll bring these data into R and explore them prior to creating a regression model.

## 4.1 Setup and Load Data

Let’s start by loading the requisite libraries and creating a `mapTheme`, reading the epa shapefile and plotting it using the native sf plotting function. Note that if you are new to the sf package you should go run-through this tutorial.

```{r libraries, message = FALSE, warning = FALSE}
library(tidyverse)
library(sf)
library(gridExtra)
library(viridis)
```

```{r mapTheme, echo=TRUE}
mapTheme <- theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  ) 
```

You can read in data in one of the following ways:

1. Put your output from the ArcGIS exercise above into the below code block:

```{r read_file, eval=FALSE}
epa <- st_read("myfilepath/myfile.shp") %>%
  st_transform(crs = 4326)
```


OR

2. Load this dataset from Github (which should match your data from the above process):

```{r read data, message = FALSE, warning = FALSE}
epa <- st_read("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_6/data/wk6_airPollution/Mid_Atlantic_EPA_Dataset_withLag.geojson")
```

## 4.2 Build some plots

Let’s build some plots using ggplot. We begin with a map of ozone. How would you modify the below code block to get just the grid cells where `training = 1`? More info on colors here.

```{r}
ggplot() +
  geom_sf(data=epa , aes(fill=ozoneHigh)) + 
  scale_fill_viridis() +
  labs(title="Ozone concentration by grid cell, Mid-Atlantic region") +
  mapTheme
```

This map is strange. It’s representing grid cells for which there is no air quality sensor as having ozone of 0.000. That’s not quite correct, however. What is the next map doing differently? Can you create this map on your own by creating two geom_sf objects? One should be just black, and a second on top that only shows our training cells.

```{r echo=FALSE}
ggplot() +
  geom_sf(data=epa, fill="black", colour = NA) +
  geom_sf(data= filter(epa, training == 1), 
          aes(fill=ozoneHigh)) + 
  scale_fill_viridis() +
  labs(title="Ozone concentration by grid cell, Mid-Atlantic region") +
  mapTheme
```

# 5 Interpolating with regression

In this section we’re going to build a regression model to predict ozone as a function of a set of independent variables or ‘features’. We start with a bit of data cleaning and exploration.

For our first step, we are going to clean up our data a bit, and add centroid locations for each cell as X and Y coordinates.

Feel free to run this code line by line to get a better sense of how it works. 

- We create a new sf object called `epa2` by piping `epa` to a series of dplyr commands.

- We use the `st_centroid` and `st_coordinates` commands from `sf` in a series of `mutate` statements. First we extract the centroid from the geometry to a column we call `centroid`, and then we use `st_coordinates` to access the ordered list of centroid coordinates - using matrix notation to call the first item in the list as the `X` and second as the `Y`. 

- We pull out the variables we want for this analysis using `select`. Examine what we keep - some variables associated with the landscape and land use, and our Ozone lag from ArcGIS - `Ave_ozoneH`.

```{r cbind_epa2, warning = FALSE, message = FALSE}
epa2 <- epa %>%
  mutate(
        centroid = st_centroid(geometry), # Compute centroid for each geometry
        X = st_coordinates(centroid)[, 1], # Extract X coordinate of the centroid
        Y = st_coordinates(centroid)[, 2]  # Extract Y coordinate of the centroid
     ) %>% 
    dplyr::select(HwyDensity,distWater,sumDevelop,sumForest,
           distI95,Population,distCities,training,
           ozoneHigh,Ave_ozoneH, X, Y)
```


Here is a map that visualizes one of our other variables, `HwyDensity`.

Can you make a second map that visualizes `sumDevelop`?

```{r}
ggplot() +
    geom_sf(data = epa2, aes(fill=HwyDensity)) + 
    scale_fill_viridis(name="Highway density") +
    mapTheme
```


## 5.1 Setup

The goal is to predict `ozoneHigh` for every grid cell in our study area. However, we only have ozone data for a subset where `training=1`. 

We will use this data to train our model. When that model is robust, we will use it to predict for the entire dataset.

Step 1 is to pull out a training set, like so.


```{r}
training <- 
  epa2 %>%
  filter(training == 1)
```


To "borrow the experience" of our known data points and predict for others, we need to harness the power of correlations between independent variables and `ozoneHigh`.

To understand these correlations, we make a "small multiple" plot where we look at the correlations between our variables and `ozoneHigh`.

This is done using the `gather` command to transform our data from wide to long form and then using scatterplots in `gglot` to look at the relationships. The `facet_wrap` command let's us make a plot for each variable - very handy.

Let's walk through this code chunk in a narrative way:

- We want to create a data frame called `scatterplot_data` that's only our independent and dependent variables, so we drop the column `training` using a `select` command and then we `st_drop_geometry` to get rid of the geometry column.

- We `gather` our data into a long data set that consists of combinations of each column variable with `ozoneHigh`. Examine these data and see how they look.

- We make a `ggplot` from this output that consists of a `geom_point` that is `ozoneHigh` as a function of the `value` column., with a `geom_smooth` best fit line through it, and the `facet_wrap` command makes one for each `variable`!


```{r}
scatterplot_data <- training %>% 
  dplyr::select(-training) %>%
  st_drop_geometry() %>%
  gather(Variable, Value, -ozoneHigh)

    ggplot(data = scatterplot_data, aes(x = Value, y = ozoneHigh)) +
      geom_point() +
      geom_smooth(method = "lm", se=FALSE) +
      facet_wrap(~Variable, scales="free", ncol=5)
```
## 5.2 Creating Models

Let’s estimate a ‘kitchen sink’ regression with all of our variables **except** the spatial variables. Note the embedded use of `select` in the `lm` command where we remove the variables we don't want.

Note that we take `training` and turn it from an `sf` to a data frame. This is because the `lm` command, wants a data frame. We use `select` to remove the variables we don't want.

***Describe the summary and its goodness of fit - what are the significant variables? What are their coefficients and what does that mean? What is the r-squared value?***

Overall, what do we make of this model?


```{r}
reg <- lm(ozoneHigh ~ ., data=training %>% 
                                   as.data.frame() %>%
                                   select(-Ave_ozoneH,-geometry,-training,-X,-Y))
summary(reg)

```

Let's estimate a new model, this time with x and y included as predictors. 

***What do you notice about their coefficients and p-values. What do you notice about the R-squared? ***

Why is this happening?


```{r}
reg2 <- lm(ozoneHigh ~ ., data=training %>% 
                                   as.data.frame() %>%
                                   select(-Ave_ozoneH,-geometry,-training))
summary(reg2)
```

## 5.3 Add the spatial lag

Let's add in our spatial lag. 

***What is a spatial lag? What would the "hypothesis" be in choosing to include this in our model?***

***What do you think about this correlation?***


```{r}
ggplot(training, aes(Ave_ozoneH, ozoneHigh)) +
 geom_point() +
 geom_smooth(method="lm",se=F) +
 labs(title="Ozone as a function of the spatial lag of ozone")
```
Let’s make a new regression called `reg.lag` that includes spatial lag in the regression.

***What happened to all the other variables and their model parameters? Why?***


```{r}
reg.lag <- lm(ozoneHigh ~ ., data=training %>% 
                                   as.data.frame() %>%
                                   select(-geometry,-training))
summary(reg.lag)
```


## 5.4 Asessing goodness of fit

R^2 is often the "go to" indicator of goodness of fit.

***Do you know why this is? How do we interpret the R^2 statistic?***

When we do predictive modeling however, R^2 can be misleading. A better way to think about error, when possible, is simply the difference between the observed value and the predicted value.

Let’s check out our predictions. 

We create three new prediction fields and convert to long form - the format we need to create small multiple plots and to do `group_by` summaries in `dplyr`.

We use the `predict` function to use our model(s) to estimate an ozone value for each cell given its combination of variables.

```{r}
training.summary <-
  training %>% 
    mutate(reg1.Pred = predict(reg, .),
           reg2.Pred = predict(reg2, .),
           reg.lag.Pred = predict(reg.lag, .)) %>%
    dplyr::select(c(ozoneHigh, starts_with("reg"))) %>%
    gather(Variable, Value, -ozoneHigh, -geometry) 

training.summary
```

Next, create three plots of predictions for observed. 

Which model looks most accurate?

```{r plot_training, message = FALSE, warning= FALSE}
ggplot(data = training.summary, aes(Value, ozoneHigh)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~Variable, scales = "free")
```

Finally, we calculate the Mean Absolute Error. 

`absError` is the absolute value of the difference between observed ozoneHigh and the predicted. Why do you think we are interested in absolute error rather than raw error?

Notice that `group_by` allows us to calculate the average error by each regression type. 

***Which model is the strongest?***

```{r}
st_drop_geometry(training.summary) %>%
  mutate(absError = abs(ozoneHigh - Value)) %>%
  group_by(Variable) %>%
  summarize(MeanAbsoluteError = mean(absError))
```
Finally, let’s predict for the entire dataset.

This set of predictions is our interpolated output.

```{r}
epa2 <- epa2 %>%
  mutate(reg.lag.pred = predict(reg.lag, epa2))

ggplot() +
  geom_sf(data=epa2, aes(fill = reg.lag.pred)) + 
  scale_fill_viridis() +
  labs(title="Predicted ozone") +
  mapTheme
```
Let’s visualize the prediction with the areas where we have air sensors (ie. training == 1). 

We use the `predict` function to predict values for `epa2` using our model `reg.lag` and the independent variables in the `epa2` data set.

```{r}

ggplot() +
  geom_sf(data=epa2, aes(fill = reg.lag.pred)) + 
  geom_sf(data = st_centroid(filter(epa2, training == 1)), colour="black") +
  scale_fill_viridis() +
  labs(title="Predicted ozone") +
  mapTheme
```

# 6. Questions

What can you conclude about the statistical reliability of our predictions given this map? 

Were you able to conduct the IDW and Kriging maneuvers in ArcGIS? If so, how do these compare? Create a sequence of maps to examine the differences!

## 6.1. Bonus section - comparing methods

**If time allows, we will compare the outputs of our IDW and Kriging in ArcGIS to our lag and our regression predictions in R.**

Let's load in our kriging and IDW predictions that we output from ArcGIS. Here are your tasks:

1. Load your kriging and idw csv files using read.csv. Call these files `krige_pred` and `idw_pred`

```{r bonus_real, include=FALSE}
idw_pred <- read.csv("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_6/data/wk6_airPollution/idw_zonal.csv")

krige_pred <- read.csv("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_6/data/wk6_airPollution/krige_zonal.csv")
```

2. Do some dplyr - `select` only columns `MEAN` and the unique fishnet cell ID from each table. Sometimes this unique ID is a little tricky - what's the right ID to get it to join to `epa`? `rename` the `MEAN` column to be `krige_pred` and `idw_pred` respectively.

```{r bonus_real_dplyr, include=FALSE}
idw_pred <- idw_pred %>%
  select(MEAN, FID) %>%
  rename(idw_pred = MEAN)

krige_pred <- krige_pred %>%
  select(MEAN, FID_) %>%
  rename(krige_pred = MEAN)
```

3. Use a tabular join

Join each data set to `epa` using the `left_join` function, like so:

```{r join_example, eval=FALSE}

new_data_set <- left_join(epa, idw_pred, by = c("left side unique ID" = "right side unique ID"))

```

```{r bonus_real_join, include=FALSE}
tables_joined<- idw_pred %>%
  left_join(., krige_pred, by = c("FID" = "FID_"))

epa_joined <- epa %>% left_join(., tables_joined, by = c("FID_1" = "FID"))
  
```

4. Can you map lag vs krige vs idw vs regression using ggplot and `grid_arrange`?

```{r ggplot_preds, echo=FALSE}
grid.arrange(ggplot() +
    geom_sf(data = epa_joined, aes(fill=Ave_ozoneH), color = "transparent") + 
    scale_fill_viridis(name="Spatial Lag") +
    mapTheme,
    ggplot() +
    geom_sf(data = epa_joined, aes(fill=idw_pred), color = "transparent") + 
    scale_fill_viridis(name="IDW") +
    mapTheme,
    ggplot() +
    geom_sf(data = epa_joined, aes(fill=krige_pred), color = "transparent") + 
    scale_fill_viridis(name="Kriging") +
    mapTheme,
    ggplot() +
  geom_sf(data=epa2, aes(fill = reg.lag.pred), color = "transparent") + 
  scale_fill_viridis(name="Regression") +
  mapTheme,
    ncol=2)
```

# Appendix 1 - Creating The Fishnet in R

The `sf` package offers really easy way to create fishnet grids - `st_make_grid`.

Let's load a spatial data file and make our very own. Remember our data from Chester County in Week 2? Let's load our Chester County municipalities file, then use `st_union` to go from a bunch of polygons to just one (e.g. the boundary).

```{r load_chesco, warning = FALSE, message = FALSE}
chesterBoundary <- read_sf("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_2/data/R_Data/Chester_MuniBoundaries.geojson") %>%
  st_union()

```

Not sure what the projection is? It's important to know - the cellsize you set for your fishnet will be in the native units of the projection. Check `st_crs(chesterBoundary)` to find out, and `st_transform` to the projection you need if necessary.

Here we are our CRS is 2272 (PA State Plane, linear unit = feet), so we're OK. We set the `cellsize = 10000` - 10,000 feet per cell.

Examine the fishnet - the unique ID is crucial to building a data set!

```{r make_fishnet}

fishnet <- 
  st_make_grid(chesterBoundary,
               cellsize = 10000, 
               square = TRUE) %>%
  .[chesterBoundary] %>%            # clips the grid to the chesterBoundary file
  st_sf() %>%
  mutate(uniqueID = rownames(.))
```


Voila, now you have a fishnet you can use for your analysis. Let's take a quick look at it and add the `chesterBoundary` for some context.

```{r plot_fishnet}

ggplot()+
  geom_sf(data = fishnet)+
  geom_sf(data = chesterBoundary, 
          color = "red", fill = "transparent")

```

# Appendix 2 - IDW and Kriging in R

Can you do Kriging or IDW in R? Of course you can!

Check out these resources to learn more:

Kriging is [usually done using an older spatial package called `sp`](https://towardsdatascience.com/building-kriging-models-in-r-b94d7c9750d8) but [here it's performed starting with an `sf` object.](https://gis.stackexchange.com/questions/287988/kriging-example-with-sf-object)
University of the Negev [has a bookdown](http://132.72.155.230:3838/r/spatial-interpolation-of-point-data.html) with spatial interpolation examples in R for both IDW and kriging.